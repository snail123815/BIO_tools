{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T07:10:15.691502Z",
     "start_time": "2018-04-23T07:10:15.685920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your email address for EntrezSearch:c.du@biology.leidenuniv.nl\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = input(\"Your email address for EntrezSearch:\")\n",
    "import os\n",
    "import pickle\n",
    "import lzma\n",
    "from datetime import date\n",
    "from time import strftime, sleep\n",
    "from urllib.error import HTTPError\n",
    "from http.client import IncompleteRead\n",
    "import sys\n",
    "from Bio import SeqIO\n",
    "from Bio import Seq\n",
    "import subprocess\n",
    "\n",
    "\n",
    "prefix = \"txid85011\"\n",
    "searchTerm = f'{prefix}[Organism] AND refseq[filter] NOT \"sequencing project\"[Title]'\n",
    "\n",
    "newSearch = False\n",
    "\n",
    "newFetch = True\n",
    "\n",
    "if newSearch:\n",
    "    dateOfSearch = date.today().strftime(\"%Y%m%d\") # For new search\n",
    "else:\n",
    "    dateOfSearch = '20180918' # For old search, change the date accrodingly\n",
    "    \n",
    "def writeLog(logFile, logStr, end = '\\n'):\n",
    "    print(logStr, end = end)\n",
    "    with open(logFile,'a') as logHandle:\n",
    "        logHandle.write(f'{logStr}{end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T13:07:29.189732Z",
     "start_time": "2018-04-20T13:07:29.185851Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Taxonomy ID: 228398 (for references in articles please use NCBI:txid228398)  \n",
    " Scientific name: Streptacidiphilus Kim et al. 2003\n",
    "* Taxonomy ID: 2063 (for references in articles please use NCBI:{prefix})  \n",
    " Scientific name: Kitasatospora corrig. Omura et al. 1983 emend. Zhang et al. 1997\n",
    "* Taxonomy ID: 1883 (for references in articles please use NCBI:txid1883)  \n",
    " Scientific name: Streptomyces Waksman and Henrici 1943 (Approved Lists 1980) emend. Wellington et al. 199* \n",
    "* Taxonomy ID: 1914443 (for references in articles please use NCBI:txid1914443)  \n",
    " Scientific name: Allostreptomyces Huang et al. 2017\n",
    "* Taxonomy ID: 329648 (for references in articles please use NCBI:txid329648)  \n",
    " Scientific name: \"Parastreptomyces\" 1) Nichols et al. 2005\n",
    "* Taxonomy ID: 65518 (for references in articles please use NCBI:txid65518)  \n",
    " Scientific name: \"Trichotomospora\" 1) Lian et al. 1985\n",
    "* Taxonomy ID: 234292 (for references in articles please use NCBI:txid234292)  \n",
    " Scientific name: unclassified Streptomycetaceae\n",
    "* Taxonomy ID: 259296 (for references in articles please use NCBI:txid259296)  \n",
    " Scientific name: environmental samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:16:59.251100Z",
     "start_time": "2018-04-22T15:15:06.303828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1>2000|\n",
      "WebAccession:\n",
      "NCID_1_143180480_130.14.18.125_9001_1537299996_1745748147_0MetA0_S_MegaStore\n",
      "Total entry 277100.\n",
      "No.2>2000|No.3>2000|No.4>2000|No.5>2000|No.6>2000|No.7>2000|No.8>2000|No.9>2000|No.10>2000|No.11>2000|No.12>2000|No.13>2000|No.14>2000|No.15>2000|No.16>2000|No.17>2000|No.18>2000|No.19>2000|No.20>2000|No.21>2000|No.22>2000|No.23>2000|No.24>2000|No.25>2000|No.26>2000|No.27>2000|No.28>2000|No.29>2000|No.30>2000|No.31>2000|No.32>2000|No.33>2000|No.34>2000|No.35>2000|No.36>2000|No.37>2000|No.38>2000|No.39>2000|No.40>2000|No.41>2000|No.42>2000|No.43>2000|No.44>2000|No.45>2000|No.46>2000|No.47>2000|No.48>2000|No.49>2000|No.50>2000|No.51>2000|No.52>2000|No.53>2000|No.54>2000|No.55>2000|No.56>2000|No.57>2000|No.58>2000|No.59>2000|No.60>2000|No.61>2000|No.62>2000|No.63>2000|No.64>2000|No.65>2000|No.66>2000|No.67>2000|No.68>2000|No.69>2000|No.70>2000|No.71>2000|No.72>2000|No.73>2000|No.74>2000|No.75>2000|No.76>2000|No.77>2000|No.78>2000|No.79>2000|No.80>2000|No.81>2000|No.82>2000|No.83>2000|No.84>2000|No.85>2000|No.86>2000|No.87>2000|No.88>2000|No.89>2000|No.90>2000|No.91>2000|No.92>2000|No.93>2000|No.94>2000|No.95>2000|No.96>2000|No.97>2000|No.98>2000|No.99>2000|No.100>2000|No.101>2000|No.102>2000|No.103>2000|No.104>2000|No.105>2000|No.106>2000|No.107>2000|No.108>2000|No.109>2000|No.110>2000|No.111>2000|No.112>2000|No.113>2000|No.114>2000|No.115>2000|No.116>2000|No.117>2000|No.118>2000|No.119>2000|No.120>2000|No.121>2000|No.122>2000|No.123>2000|No.124>2000|No.125>2000|No.126>2000|No.127>2000|No.128>2000|No.129>2000|No.130>2000|No.131>2000|No.132>2000|No.133>2000|No.134>2000|No.135>2000|No.136>2000|No.137>2000|No.138>2000|No.139>1100|\n",
      "Total target sets 139\n",
      "First 10:\n",
      "['1468450368', '1462727958', '1462726371', '1462726109', '1462725418']...\n",
      "['1431100726', '1431100725', '1431100724', '1431100723', '1431100722']...\n",
      "['1424197081', '1424197080', '1424197079', '1424197078', '1424197077']...\n",
      "['1407021246', '1407021245', '1407021244', '1407021243', '1407021242']...\n",
      "['910050850', '910050845', '910050839', '910050836', '910050831']...\n",
      "['1393770146', '1393770145', '1393770144', '1393770143', '1393770142']...\n",
      "['1393726806', '1393726805', '1393726804', '1393726803', '1393726802']...\n",
      "['1268590657', '1268590656', '1268590655', '1268590654', '1268590653']...\n",
      "['1376470493', '1376470492', '1376470491', '1376470490', '1376470489']...\n",
      "['1341296649', '1341296648', '1341296647', '1341296646', '1341296645']...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def getRecords(searchTerm, c = 0, step = 2000, webaccession = None):\n",
    "    targets = []\n",
    "    count = 0\n",
    "    while c < count or count == 0:\n",
    "        print(f'No.{int(c/step+1)}', end = '>')\n",
    "        handle = Entrez.esearch(db='nuccore', \n",
    "                                term=searchTerm,\n",
    "                                usehistory=True,\n",
    "                                webenv=webaccession, # reuse teh first query\n",
    "                                retstart=c, # Continue from num c\n",
    "                                retmax=step # Maxium number returned\n",
    "                               )\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        if record['Count'] == \"0\":\n",
    "            print(\"\\nThere is no genome with this search.\")\n",
    "            break\n",
    "        if 'WarningList' in record: # If anything wrong\n",
    "            print(record['WarningList']['OutputMessage'])\n",
    "\n",
    "        # Put current list of IDs in our targets dict    \n",
    "        targets.append(record['IdList'])\n",
    "        print(f\"{len(record['IdList'])}\", end = '|')  # the number got from this loop.\n",
    "\n",
    "        if webaccession == None:\n",
    "            webaccession = record[\"WebEnv\"] # start new search only in the first loop\n",
    "            print(f'\\nWebAccession:\\n{webaccession}')\n",
    "            count = int(record['Count']) # total number of hits get from the first search attempt\n",
    "            print(f\"Total entry {count}.\")    \n",
    "        c += step\n",
    "    return c, count, webaccession, targets\n",
    "\n",
    "\n",
    "c, count, webaccession, targets = getRecords(searchTerm = searchTerm)\n",
    "\n",
    "# now print and check the first 5 IDs of the first 10 lists of IDs\n",
    "print(f\"\\nTotal target sets {len(targets)}\\nFirst 10:\")\n",
    "for targ in targets[:10]:\n",
    "    print(f\"{targ[:5]}...\")\n",
    "print('...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching all data from online (getting IDs from previous stored data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T17:45:01.238923Z",
     "start_time": "2018-04-22T17:45:00.238202Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nucids 277100\n",
      "Remember to switch off <newSearch>\n"
     ]
    }
   ],
   "source": [
    "# Dump targets got from search in pickle file\n",
    "pickleFiles = r\"C:\\Users\\duc\\Genomes\"\n",
    "targetsPickle = os.path.join(pickleFiles, f'refseq_{prefix}_{dateOfSearch}.pickle.xz')\n",
    "if newSearch:\n",
    "    with lzma.open(targetsPickle, 'wb') as pickleOut:\n",
    "        pickle.dump(targets, pickleOut)\n",
    "    newSearch = False # switch off newSearch\n",
    "else:\n",
    "    with lzma.open(targetsPickle, 'rb') as pickleIn:\n",
    "        targets = pickle.load(pickleIn)\n",
    "        \n",
    "# flatten the targets list of list\n",
    "# remove redundant if we have somene\n",
    "\n",
    "nucids = sorted(list(set(item for sublist in targets for item in sublist)))\n",
    "\n",
    "print(f\"Total nucids {len(nucids)}\\nRemember to switch off <newSearch>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T17:45:01.763480Z",
     "start_time": "2018-04-22T17:45:01.756241Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch(index, file_Nu, ids, returnType = ['fasta','fasta']):\n",
    "    '''fetch(index, file_Nu, ids, returnType = ['fasta','fasta'])\n",
    "    \n",
    "    other option is:\n",
    "    returnType = ['gbwithparts','gb']'''\n",
    "\n",
    "    writeLog(logFile, f\"Fetching {file_Nu}: {ids[:4]}...({len(ids)})\")\n",
    "    \n",
    "    output_file = os.path.join(outputPath, f'{prefix}_No_{file_Nu}.{returnType[1]}')\n",
    "\n",
    "    # Fetching...\n",
    "    with Entrez.efetch(db = 'nuccore',\n",
    "                       id = ids,\n",
    "                       rettype = returnType[0],\n",
    "                       retmode = 'text'\n",
    "                      ) as handle:\n",
    "        with open(output_file, 'w') as out_handle:\n",
    "            out_handle.write(handle.read())\n",
    "    \n",
    "    # Write finishing note to screen and log file\n",
    "    logstr = f\"Finished {file_Nu}: {os.stat(output_file).st_size/1024/1024:.2f} MB {output_file} \\n\"\n",
    "    print(logstr)\n",
    "    with open(logFile,'a') as log_handle:\n",
    "        log_handle.write(f'{logstr}\\n')\n",
    "    return True\n",
    "\n",
    "\n",
    "def finishing(allDone = True):\n",
    "    totalSize = 0 # calculate total amount data got from entrez\n",
    "    for file in os.listdir(outputPath):\n",
    "        if file.endswith('.gb') or file.endswith('.fasta'):\n",
    "            totalSize += os.stat(os.path.join(outputPath,file)).st_size\n",
    "    \n",
    "    writeLog(logFile, f\"Group finished, already got {totalSize/1024/1024:.2f} MB data!\")\n",
    "    if allDone:\n",
    "            writeLog(logFile,'Finished fetching all IDs.')\n",
    "    else:\n",
    "        writeLog(logFile, f'Next download will start from group {start} (of 0 - {len(idStacks)-1}).')\n",
    "        writeLog(logFile, f'{timestamp:*^80}')\n",
    "        print('Before starting next query, please set how many groups you want to fetch based on your schedule.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T06:57:20.503324Z",
     "start_time": "2018-04-23T06:57:20.500259Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 663 # should start from 0 if nothing have downloaded\n",
    "getSingleBatch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T06:57:44.861731Z",
     "start_time": "2018-04-23T06:57:38.725340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last group No. 0923\n",
      "\n",
      "******************21:33:31 21/09/2018 W. Europe Daylight Time*******************\n",
      "Now fetching groups from 663 to 923\n",
      "Each group have 300 nuclIDs\n",
      "IDs from 198901 to 277200 (inclusive)\n",
      "********************************************************************************\n",
      "\n",
      "Fetching 0663: ['739837141', '739837154', '739837165', '739837183']...(300)\n",
      "Finished 0663: 68.62 MB C:\\Users\\duc\\Genomes\\GBKs\\txid85011_No_0663.gb \n",
      "\n"
     ]
    }
   ],
   "source": [
    "downloadStep = 300 # this should not be changed during download\n",
    "\n",
    "idStacks = []\n",
    "for i in range(0,len(nucids),downloadStep):\n",
    "    idStacks.append(nucids[i:i+downloadStep])\n",
    "\n",
    "print(f'Last group No. {str(len(idStacks)-1).zfill(4)}')\n",
    "\n",
    "numToFetch = 999 # number of files to fetch (specifing if you can not finish in one go)\n",
    "\n",
    "end = start + numToFetch # range(start, end) means not including end number!!\n",
    "\n",
    "if end > len(idStacks):\n",
    "    end = len(idStacks)\n",
    "\n",
    "outputPath = r\"C:\\Users\\duc\\Genomes\\GBKs\"\n",
    "logFile = os.path.join(outputPath, f'fetching.log')\n",
    "timestamp = strftime('%X %d/%m/%Y %Z')\n",
    "\n",
    "writeLog(logFile, f'''\n",
    "{timestamp:*^80}\n",
    "Now fetching groups from {start} to {end-1}\n",
    "Each group have {downloadStep} nuclIDs\n",
    "IDs from {start*downloadStep+1} to {end*downloadStep+1-1} (inclusive)\n",
    "{\"\":*^80}\n",
    "''')\n",
    "# Mean loop\n",
    "for i in range(start, len(idStacks)):\n",
    "    # Break the loop for shorter operation and debugging time\n",
    "    if i == end and i != len(idStacks)-1:\n",
    "        start = i\n",
    "        finishing(allDone = False)\n",
    "        break\n",
    "        \n",
    "    ids = idStacks[i]\n",
    "    \n",
    "    succeed = False\n",
    "    retryTimes = 0\n",
    "    while succeed == False:\n",
    "        try:\n",
    "            succeed = fetch(index = i, file_Nu = str(i).zfill(4), ids = ids, returnType = ['gbwithparts','gb'])\n",
    "        except HTTPError:\n",
    "            retryTimes += 1\n",
    "            if retryTimes == 3:\n",
    "                print(\"Failed 3 times due to HTTPError (NCBI server problem), please try again another time.\")\n",
    "                start = i\n",
    "                print(f'Start value set for next trial. [{start}]')\n",
    "                break\n",
    "            else:\n",
    "                print('Failed due to HTTPError, pause for 45 seconds...',end = '')\n",
    "                sleep(45)\n",
    "                print('Retrying...')\n",
    "        except IncompleteRead:\n",
    "            start = i\n",
    "            print(f'Failed due to connection loss, start value set for next trial. [{start}]') \n",
    "            break\n",
    "        except:\n",
    "            start = i\n",
    "            print(f'Failed due to unknown error, start value set for next trial. [{start}]') \n",
    "            print(f'{sys.exc_info()}')\n",
    "            break\n",
    "    if not succeed:\n",
    "        break\n",
    "    elif i == len(idStacks) - 1:\n",
    "        finishing(allDone = True)\n",
    "    if getSingleBatch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert genbank file to blast database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gb files needs to be converet to fasta file before making a database  \n",
    "also the files needed to be combined incase combine blast database don't work, also for easy file transfering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T19:15:27.766298Z",
     "start_time": "2018-04-20T18:53:50.010911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convertGb2Fasta(storage_folder_gb, storage_folder_fa, logFile, sizeLimit, numFilesToConvert = 9999):\n",
    "    print('Changing gb to fasta...')\n",
    "    \n",
    "    totalGbs = sum(file.endswith('gb') for file in os.listdir(storage_folder_gb))\n",
    "\n",
    "    # write log file\n",
    "    writeLog(logFile, f'\\n{timestamp:*^50}\\n{totalGbs} of gb files in total\\n{\"\":*^50}\\n')\n",
    "    # setup starting values\n",
    "    recordWithSeq = [] # container for seqs for the function of dumping certain size of seqs together.\n",
    "    seqTotalLength = 0\n",
    "    num_empty = 0\n",
    "    breakPoint = 0\n",
    "    \n",
    "    # Main loop\n",
    "    for i in range(totalGbs):\n",
    "        num = str(i).zfill(4)\n",
    "        input_file = os.path.join(storage_folder_gb, f'{prefix}_No_{num}.gb') \n",
    "        # If there is a gap in the input file, it will currupt here.\n",
    "\n",
    "        # Write note to screen and log file\n",
    "        writeLog(logFile, f\"{num}|{seqTotalLength}\", end = '\\t')\n",
    "        \n",
    "        records = SeqIO.parse(input_file, 'genbank')\n",
    "        for record in records:\n",
    "            if type(record.seq) == Seq.UnknownSeq: # Empty records will load as UnknownSeq\n",
    "                num_empty += 1\n",
    "                pass\n",
    "            else:\n",
    "                recordWithSeq.append(record)\n",
    "                seqTotalLength += len(record)\n",
    "\n",
    "        if i == end - 1:\n",
    "            print('\\nReach the end, writing it out...')\n",
    "            output_file = os.path.join(storage_folder_fa, f'{prefix}_No_{breakPoint}to{num}.fasta')\n",
    "            with open(output_file, 'w') as fasta_out_handle:\n",
    "                if len(recordWithSeq) == 0: # if all records are empty, there is no point of writing it to fasta\n",
    "                    writeLog(logFile, f'There is no sequence in this batch, proceed to next file...')\n",
    "                else:\n",
    "                    SeqIO.write(recordWithSeq, fasta_out_handle, 'fasta')\n",
    "                    writeLog(logFile, f\"Finished converting {num}, ignored {num_empty} empty records in this batch.\")\n",
    "        elif seqTotalLength >= sizeLimit:\n",
    "            print('\\nReach the size limit of single file, writing it out')\n",
    "            output_file = os.path.join(storage_folder_fa, f'{prefix}_No_{breakPoint}to{num}.fasta')\n",
    "            with open(output_file, 'w') as fasta_out_handle:\n",
    "                if len(recordWithSeq) == 0: # if all records are empty, there is no point of writing it to fasta\n",
    "                    writeLog(logFile, f'There is no sequence in this batch, proceed to next file...')\n",
    "                else:\n",
    "                    SeqIO.write(recordWithSeq, fasta_out_handle, 'fasta')\n",
    "                    writeLog(logFile, f\"Finished converting {num}, ignored {num_empty} empty records in this batch.\")\n",
    "            # Reset batch\n",
    "            recordWithSeq = list()\n",
    "            seqTotalLength = 0\n",
    "            num_empty = 0\n",
    "            breakPoint = str(i).zfill(4)\n",
    "            \n",
    "            \n",
    "def combineFasta(fetchedFasta,combinedFasta,logFile,sizeLimit):\n",
    "    if not os.path.isdir(combinedFasta):\n",
    "        os.mkdir(combinedFasta)\n",
    "\n",
    "    fileList = [os.path.join(fetchedFasta, file) for file in os.listdir(fetchedFasta) if file.endswith('.fasta')]\n",
    "    seqTotalLength = 0\n",
    "    seqsInOneFile = []\n",
    "    breakPoint = 0\n",
    "    for i in range(len(fileList)):\n",
    "        parseSeqs = list(SeqIO.parse(fileList[i],'fasta'))\n",
    "        for seq in parseSeqs:\n",
    "            seqTotalLength += len(seq)\n",
    "            if len(seq) == 0:\n",
    "                print('There is 0 length sequences')\n",
    "                break\n",
    "        seqsInOneFile += parseSeqs\n",
    "        logStr = f\"{i}|{seqTotalLength}\"\n",
    "        writeLog(logFile, logStr, end = '\\t')\n",
    "\n",
    "        if i == len(fileList) - 1:\n",
    "            outputFile = os.path.join(combinedFasta, f'{prefix}_{breakPoint}_to_{i}.fasta')\n",
    "            logStr = f'\\nFinished combining, write out last batch...\\n{outputFile}'\n",
    "            writeLog(logFile, logStr)\n",
    "            SeqIO.write(seqsInOneFile, outputFile, 'fasta')\n",
    "            writeLog(logFile, 'Write succeeded.')\n",
    "        elif seqTotalLength >= sizeLimit:\n",
    "            outputFile = os.path.join(combinedFasta, f'{prefix}_{breakPoint}_to_{i}.fasta')\n",
    "            logStr = f'\\nReached file size limit, write out...\\n{outputFile}'\n",
    "            writeLog(logFile, logStr)\n",
    "            SeqIO.write(seqsInOneFile, outputFile, 'fasta')\n",
    "            breakPoint = i+1\n",
    "            seqsInOneFile = []\n",
    "            seqTotalLength = 0\n",
    "            writeLog(logFile, 'Write succeeded.')\n",
    "\n",
    "            \n",
    "timestamp = strftime('%X %d/%m/%Y %Z')\n",
    "fastaFolder = '/Users/durand.dc/Desktop/downloadingGenomes/fasta/'\n",
    "fetched = '/Users/durand.dc/Desktop/downloadingGenomes/'\n",
    "logFile = os.path.join(fastaFolder,'converting.log')\n",
    "print('Converting...')\n",
    "sizeLimit = 1000000000\n",
    "\n",
    "if sum(file.endswith('gb') for file in os.listdir(fetched)) == 0:\n",
    "    combineFasta(fetched,fastaFolder,logFile,sizeLimit)\n",
    "else:\n",
    "    convertGb2Fasta(fetched,fastaFolder,logFile,sizeLimit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting...\n",
      "624|20489360\t625|33104520\t626|49716308\t627|61162517\t628|61285205\t629|61382266\t630|80394401\t631|99518807\t632|105539401\t633|121175321\t634|130980238\t635|159795408\t636|181099523\t637|207276554\t638|223738909\t639|242031150\t640|254809612\t641|266286263\t642|266499318\t643|274355341\t644|296091288\t645|304958311\t646|313531238\t647|330438723\t648|342390697\t649|359738852\t650|370659660\t651|383294104\t652|393047243\t653|398910373\t654|406814455\t655|409714728\t656|416051914\t657|427343023\t658|493599200\t659|524767428\t660|537291227\t661|553778644\t662|554819446\t663|585463317\t664|622213627\t665|640884943\t666|669689697\t667|703131549\t668|766916561\t669|805876513\t670|823980294\t671|843066391\t672|861945725\t673|870254739\t674|879094673\t675|921988168\t676|948024312\t677|993323582\t678|998756726\t679|1010820404\t\n",
      "Reached file size limit, write out...\n",
      "C:\\Users\\duc\\Genomes\\GBKs\\combined\\txid85011_624_to_679.gb\n",
      "Write succeeded.\n",
      "680|8601199\t681|28212280\t682|31442734\t683|36469972\t684|39668096\t685|42329040\t686|44164701\t687|45314265\t688|45953602\t689|48280620\t690|50125072\t691|51639840\t692|52742824\t693|53436295\t694|53720224\t695|62348164\t696|68071688\t697|76620676\t698|79307329\t699|79445833\t700|81174975\t701|85064465\t702|87585280\t703|88954312\t704|94878409\t705|96794810\t706|101967597\t707|104972493\t708|106421924\t709|108912510\t710|112931193\t711|115314331\t712|116514842\t713|128587784\t714|132396732\t715|134053814\t716|164468205\t717|188781099\t718|195271488\t719|205033067\t720|214600664\t721|229658282\t722|232513594\t723|234572202\t724|238247257\t725|240427999\t726|241615783\t727|242142836\t728|256168632\t729|272694427\t730|308228221\t731|333885160\t732|340301333\t733|351168938\t734|355861253\t735|366804638\t736|378125886\t737|383383976\t738|390893570\t739|400244623\t740|409654687\t741|442170277\t742|452036048\t743|466390825\t744|475401105\t745|482776920\t746|493047072\t747|500527628\t748|512970540\t749|521285373\t750|526544377\t751|532211322\t752|542060405\t753|553407619\t754|561701382\t755|570718254\t756|580662231\t757|588546110\t758|600066152\t759|610447290\t760|618429567\t761|622904300\t762|633288910\t763|645688036\t764|652593554\t765|661320994\t766|670686609\t767|679150882\t768|687103003\t769|698748276\t770|709086988\t771|714391464\t772|723620150\t773|729589907\t774|740056304\t775|744936146\t776|756866436\t777|766037455\t778|772242596\t779|781436555\t780|788639906\t781|797393068\t782|806774119\t783|816176284\t784|822942437\t785|830307969\t786|832915509\t787|844186145\t788|856286978\t789|875775859\t790|877948532\t791|892322879\t792|898026540\t793|900481212\t794|903492760\t795|907429770\t796|910959072\t797|916710930\t798|920013503\t799|923994453\t800|926149959\t801|927038093\t802|932798435\t803|935687771\t804|937323291\t805|940407163\t806|942791085\t807|944385155\t808|945435212\t809|949609335\t810|954081013\t811|955546989\t812|968499608\t813|971445443\t814|973490467\t815|979117673\t816|981175710\t817|983792577\t818|985974277\t819|987260952\t820|989428017\t821|992230650\t822|994028393\t823|996095147\t824|1000720576\t\n",
      "Reached file size limit, write out...\n",
      "C:\\Users\\duc\\Genomes\\GBKs\\combined\\txid85011_680_to_824.gb\n",
      "Write succeeded.\n",
      "825|2330205\t826|3640643\t827|8574109\t828|10825222\t829|13704901\t830|17911956\t831|19728112\t832|24394912\t833|27191635\t834|30239815\t835|33774707\t836|35336257\t837|42388134\t838|45877624\t839|55470987\t840|59825894\t841|62069056\t842|65744454\t843|69719567\t844|71792565\t845|82922721\t846|86404659\t847|88337464\t848|90284533\t849|92815118\t850|94296131\t851|95154092\t852|100752622\t853|103512706\t854|108026593\t855|111944418\t856|114777070\t857|118898497\t858|121388972\t859|122519466\t860|125630675\t861|128299331\t862|130293675\t863|131499491\t864|134585847\t865|199765126\t866|278374468\t867|319169145\t868|334591363\t869|355478774\t870|360247330\t871|369816683\t872|379751143\t873|408757683\t874|437468374\t875|462695351\t876|478125146\t877|490203657\t878|497099405\t879|498818248\t880|500504798\t881|502051955\t882|503523352\t883|505100318\t884|507999878\t885|510126716\t886|512302041\t887|514152633\t888|516020031\t889|517227850\t890|518797515\t891|520208384\t892|521482535\t893|522650172\t894|525181708\t895|527710352\t896|530195823\t897|532356918\t898|534181824\t899|536304221\t900|538433672\t901|540247973\t902|541968372\t903|543600111\t904|545328725\t905|548099651\t906|551258717\t907|554606446\t908|557239927\t909|562172347\t910|565269023\t911|567436529\t912|569437715\t913|573164984\t914|575984969\t915|578486317\t916|581429924\t917|583909102\t918|586235530\t919|588333710\t920|591286518\t921|597495612\t922|600034260\t923|610287471\t\n",
      "Finished combining, write out last batch...\n",
      "C:\\Users\\duc\\Genomes\\GBKs\\combined\\txid85011_825_to_923.gb\n",
      "Write succeeded.\n"
     ]
    }
   ],
   "source": [
    "def combineGenbank(fetchedGenbank,combinedGenbank,logFile,sizeLimit,startFileNum = 0):\n",
    "    if not os.path.isdir(combinedGenbank):\n",
    "        os.mkdir(combinedGenbank)\n",
    "\n",
    "    fileList = [os.path.join(fetchedGenbank, file) for file in os.listdir(fetchedGenbank) if file.endswith('.gb')]\n",
    "    seqTotalLength = 0\n",
    "    seqsInOneFile = []\n",
    "    breakPoint = startFileNum\n",
    "    for i in range(startFileNum, len(fileList)):\n",
    "        parseSeqs = list(SeqIO.parse(fileList[i],'genbank'))\n",
    "        for seq in parseSeqs:\n",
    "            seqTotalLength += len(seq)\n",
    "            if len(seq) == 0:\n",
    "                print('There is 0 length sequences')\n",
    "                break\n",
    "        seqsInOneFile += parseSeqs\n",
    "        logStr = f\"{i}|{seqTotalLength}\"\n",
    "        writeLog(logFile, logStr, end = '\\t')\n",
    "\n",
    "        if i == len(fileList) - 1:\n",
    "            outputFile = os.path.join(combinedGenbank, f'{prefix}_{breakPoint}_to_{i}.gb')\n",
    "            logStr = f'\\nFinished combining, write out last batch...\\n{outputFile}'\n",
    "            writeLog(logFile, logStr)\n",
    "            SeqIO.write(seqsInOneFile, outputFile, 'genbank')\n",
    "            writeLog(logFile, 'Write succeeded.')\n",
    "        elif seqTotalLength >= sizeLimit:\n",
    "            outputFile = os.path.join(combinedGenbank, f'{prefix}_{breakPoint}_to_{i}.gb')\n",
    "            logStr = f'\\nReached file size limit, write out...\\n{outputFile}'\n",
    "            writeLog(logFile, logStr)\n",
    "            SeqIO.write(seqsInOneFile, outputFile, 'genbank')\n",
    "            breakPoint = i+1\n",
    "            seqsInOneFile = []\n",
    "            seqTotalLength = 0\n",
    "            writeLog(logFile, 'Write succeeded.')\n",
    "\n",
    "            \n",
    "timestamp = strftime('%X %d/%m/%Y %Z')\n",
    "combinedFolder = r\"C:\\Users\\duc\\Genomes\\GBKs\\combined\"\n",
    "sourceFolder = r\"C:\\Users\\duc\\Genomes\\GBKs\"\n",
    "logFile = os.path.join(combinedFolder,'converting.log')\n",
    "print('Converting...')\n",
    "sizeLimit = 1000000000\n",
    "\n",
    "# 1000000000 cost about 4-6 GB memory for genbank files, result ~2.5 GB file.\n",
    "startFileNum = 624\n",
    "combineGenbank(sourceFolder,combinedFolder,logFile,sizeLimit,startFileNum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make blast database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T07:18:46.911584Z",
     "start_time": "2018-04-23T07:17:43.682288Z"
    }
   },
   "outputs": [],
   "source": [
    "sourceDir = '/Users/durand.dc/Desktop/downloadingGenomes/fasta'\n",
    "outputDir = '/Users/durand.dc/Desktop/downloadingGenomes/blastdb/'\n",
    "logFile = '/Users/durand.dc/Desktop/downloadingGenomes/blastdb/makeblastdb.log'\n",
    "\n",
    "if not os.path.isdir(outputDir):\n",
    "    os.mkdir(outputDir)\n",
    "    \n",
    "# Change here accroding to the format fetched from entrez \n",
    "totalNum = sum(file.endswith('fasta') for file in os.listdir(sourceDir))\n",
    "print(f'Total number of files to be converted {totalNum}.\\nConverted:', end='')\n",
    "\n",
    "converted = 0\n",
    "for file in os.listdir(sourceDir):\n",
    "    if not file.endswith('fasta'):\n",
    "        continue\n",
    "    singleSeqFile = os.path.join(sourceDir, file)\n",
    "    args = ['makeblastdb',\n",
    "            '-in', singleSeqFile,\n",
    "            '-input_type', 'fasta',\n",
    "            '-dbtype', 'nucl',\n",
    "            '-title', f'{file[:-6]}',\n",
    "            '-out', os.path.join(outputDir, f'{file[:-6]}'),\n",
    "            '-logfile', logFile,\n",
    "            '-taxid', '85011'\n",
    "           ]\n",
    "    run = subprocess.run(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    if run.returncode == 0:\n",
    "        converted += 1\n",
    "        print(f'{converted}|', end = '')\n",
    "        pass\n",
    "    else:\n",
    "        with open(logFile, 'r') as log:\n",
    "            for line in log.readlines():\n",
    "                print(line)\n",
    "        break\n",
    "print(f'\\n\\nFinished, databases made: {converted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge blast database into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T07:19:03.937275Z",
     "start_time": "2018-04-23T07:19:03.869547Z"
    }
   },
   "outputs": [],
   "source": [
    "databaseDir = '/Users/durand.dc/Desktop/downloadingGenomes/blastdb/'\n",
    "listFile = '/Users/durand.dc/Desktop/downloadingGenomes/blastdb/listofdbs.txt'\n",
    "databaseList = []\n",
    "for file in os.listdir(databaseDir):\n",
    "    if not file.endswith('nsq'):\n",
    "        continue\n",
    "    dbName = file.split('.')[0]\n",
    "    databaseList.append(os.path.join(databaseDir, dbName))\n",
    "with open(listFile, 'w') as handle:\n",
    "    handle.write('\\n'.join(databaseList))\n",
    "    \n",
    "args = ['blastdb_aliastool',\n",
    "        '-dblist_file', listFile,\n",
    "        '-dbtype', 'nucl',\n",
    "        '-out', os.path.join(databaseDir, f'{prefix}Nucl{dateOfSearch}'),\n",
    "        '-title', f'{prefix}Nucl{dateOfSearch}',\n",
    "       ]\n",
    "run = subprocess.run(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "if run.returncode == 0:\n",
    "    pass\n",
    "else:\n",
    "    print(run.stdout.decode())\n",
    "    print(run.stderr.decode())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {
    "height": "267px",
    "width": "353px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "366px",
    "left": "1115.71875px",
    "right": "20.28125px",
    "top": "120px",
    "width": "429px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "594px",
    "left": "947px",
    "right": "24px",
    "top": "75px",
    "width": "594px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
