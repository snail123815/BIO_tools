{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:07:45.693732Z",
     "start_time": "2017-09-02T19:07:45.410530Z"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = 'c.du@biology.leidenuniv.nl'\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:07:45.714976Z",
     "start_time": "2017-09-02T19:07:45.697986Z"
    }
   },
   "outputs": [],
   "source": [
    "targets = [] # target ID storage\n",
    "count = 0    #  \n",
    "c = 0\n",
    "step = 2000\n",
    "webaccession = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:09:31.218495Z",
     "start_time": "2017-09-02T19:07:47.096687Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1>2000|\n",
      "WebAccession:\n",
      "NCID_1_77863634_130.14.18.34_9001_1537132678_1127887493_0MetA0_S_MegaStore\n",
      "Total entry 278135.\n",
      "No.2>2000|No.3>2000|No.4>2000|No.5>2000|No.6>2000|No.7>2000|No.8>2000|No.9>2000|No.10>2000|No.11>2000|No.12>2000|No.13>2000|No.14>2000|No.15>2000|No.16>2000|No.17>2000|No.18>2000|No.19>2000|No.20>2000|No.21>2000|No.22>2000|No.23>2000|No.24>2000|No.25>2000|No.26>2000|No.27>2000|No.28>2000|No.29>2000|No.30>2000|No.31>2000|No.32>2000|No.33>2000|No.34>2000|No.35>2000|No.36>2000|No.37>2000|No.38>2000|No.39>2000|No.40>2000|No.41>2000|No.42>2000|No.43>2000|No.44>2000|No.45>2000|No.46>2000|No.47>2000|No.48>2000|No.49>2000|No.50>2000|No.51>2000|No.52>2000|No.53>2000|No.54>2000|No.55>2000|No.56>2000|No.57>2000|No.58>2000|No.59>2000|No.60>2000|No.61>2000|No.62>2000|No.63>2000|No.64>2000|No.65>2000|No.66>2000|No.67>2000|No.68>2000|No.69>2000|No.70>2000|No.71>2000|No.72>2000|No.73>2000|No.74>2000|No.75>2000|No.76>2000|No.77>2000|No.78>2000|No.79>2000|No.80>2000|No.81>2000|No.82>2000|No.83>2000|No.84>2000|No.85>2000|No.86>2000|No.87>2000|No.88>2000|No.89>2000|No.90>2000|No.91>2000|No.92>2000|No.93>2000|No.94>2000|No.95>2000|No.96>2000|No.97>2000|No.98>2000|No.99>2000|No.100>2000|No.101>2000|No.102>2000|No.103>2000|No.104>2000|No.105>2000|No.106>2000|No.107>2000|No.108>2000|No.109>2000|No.110>2000|No.111>2000|No.112>2000|No.113>2000|No.114>2000|No.115>2000|No.116>2000|No.117>2000|No.118>2000|No.119>2000|No.120>2000|No.121>2000|No.122>2000|No.123>2000|No.124>2000|No.125>2000|No.126>2000|No.127>2000|No.128>2000|No.129>2000|No.130>2000|No.131>2000|No.132>2000|No.133>2000|No.134>2000|No.135>2000|No.136>2000|No.137>2000|No.138>2000|No.139>2000|No.140>135|\n",
      "Total target sets 140\n",
      "First 10:\n",
      "['1468450368', '1462728401', '1462727958', '1462726371', '1462726109']...\n",
      "['1431100747', '1431100746', '1431100745', '1431100744', '1431100743']...\n",
      "['1424197108', '1424197107', '1424197106', '1424197105', '1424197104']...\n",
      "['1407021291', '1407021290', '1407021289', '1407021288', '1407021287']...\n",
      "['910050906', '910050905', '910050904', '910050903', '910050902']...\n",
      "['1393770205', '1393770204', '1393770203', '1393770202', '1393770201']...\n",
      "['1393726866', '1393726865', '1393726864', '1393726863', '1393726862']...\n",
      "['1269662105', '1269662104', '1269662103', '1269662102', '1269662101']...\n",
      "['1376470585', '1376470584', '1376470583', '1376470582', '1376470581']...\n",
      "['1341332814', '1341332813', '1341332812', '1341332811', '1341332810']...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "while c < count or count == 0:\n",
    "    print(f'No.{int(c/step+1)}', end = '>')\n",
    "    handle = Entrez.esearch(db='nuccore', \n",
    "                            term='txid85011[Organism] AND refseq[filter]',\n",
    "                            # txid85011 = Lineage (full): \n",
    "                            #     root; cellular organisms; Bacteria;\n",
    "                            #         Terrabacteria group; Actinobacteria; Actinobacteria\n",
    "                            usehistory=True,\n",
    "                            webenv=webaccession, # reuse teh first query\n",
    "                            retstart=c, # Continue from num c\n",
    "                            retmax=step # Maxium number returned\n",
    "                           )\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    if 'WarningList' in record: # If anything wrong I could know\n",
    "        print(record['WarningList']['OutputMessage'])\n",
    "    \n",
    "    # Put current list of IDs in our targets dict    \n",
    "    targets.append(record['IdList'])\n",
    "    print(f\"{len(record['IdList'])}\", end = '|')  # the number got from this loop.\n",
    "    \n",
    "    if webaccession == None:\n",
    "        webaccession = record[\"WebEnv\"] # start new search only in the first loop\n",
    "        print(f'\\nWebAccession:\\n{webaccession}')\n",
    "        count = int(record['Count']) # total number of hits get from the first search attempt\n",
    "        print(f\"Total entry {count}.\")    \n",
    "\n",
    "    c += step\n",
    "    \n",
    "#     if c > step*2:\n",
    "#         break\n",
    "\n",
    "# now print and check the first 5 IDs of the first 10 lists of IDs\n",
    "print(f\"\\nTotal target sets {len(targets)}\\nFirst 10:\")\n",
    "for targ in targets[:10]:\n",
    "    print(f\"{targ[:5]}...\")\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching all data from online (getting IDs from previous stored data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:09:51.544198Z",
     "start_time": "2017-09-02T19:09:51.533014Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import lzma\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:10:04.938035Z",
     "start_time": "2017-09-02T19:09:52.857373Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refseq_strep_20180916.pickle.xz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump targets got from search in pickle file\n",
    "targetsPickle = f'refseq_strep_{date.today().strftime(\"%Y%m%d\")}.pickle.xz'\n",
    "with lzma.open(targetsPickle, 'wb') as output:\n",
    "    pickle.dump(targets, output)\n",
    "targetsPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T20:36:56.335209Z",
     "start_time": "2017-09-01T20:36:54.576552Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'refseq_strep_20170902.pickle.xz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-81c1fe775d83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read data from last search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdateOfSearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20170902\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mlzma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'refseq_strep_{dateOfSearch}.pickle.xz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Biopython\\lib\\lzma.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, format, check, preset, filters, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     binary_file = LZMAFile(filename, lz_mode, format=format, check=check,\n\u001b[1;32m--> 302\u001b[1;33m                            preset=preset, filters=filters)\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"t\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Biopython\\lib\\lzma.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, format, check, preset, filters)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'refseq_strep_20170902.pickle.xz'"
     ]
    }
   ],
   "source": [
    "# Read data from last search\n",
    "dateOfSearch = 20170902\n",
    "with lzma.open(f'refseq_strep_{dateOfSearch}.pickle.xz', 'rb') as pickle_in:\n",
    "    targets = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-02T19:10:24.233716Z",
     "start_time": "2017-09-02T19:10:23.904956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1|No.2|No.3|No.4|No.5|No.6|No.7|No.8|No.9|No.10|No.11|No.12|No.13|No.14|No.15|No.16|No.17|No.18|No.19|No.20|No.21|No.22|No.23|No.24|No.25|No.26|No.27|No.28|No.29|No.30|No.31|No.32|No.33|No.34|No.35|No.36|No.37|No.38|No.39|No.40|No.41|No.42|No.43|No.44|No.45|No.46|No.47|No.48|No.49|No.50|No.51|No.52|No.53|No.54|No.55|No.56|No.57|No.58|No.59|No.60|No.61|No.62|No.63|No.64|No.65|No.66|No.67|No.68|No.69|No.70|No.71|No.72|No.73|No.74|No.75|No.76|No.77|No.78|No.79|No.80|No.81|No.82|No.83|No.84|No.85|No.86|No.87|No.88|No.89|No.90|No.91|No.92|No.93|No.94|No.95|No.96|No.97|No.98|No.99|No.100|No.101|No.102|No.103|No.104|No.105|No.106|No.107|No.108|No.109|No.110|No.111|No.112|No.113|No.114|No.115|No.116|No.117|\n",
      "Total nucids 232772\n"
     ]
    }
   ],
   "source": [
    "nucids = []\n",
    "total_nucid = 0\n",
    "\n",
    "for i in range(len(targets)): # flatten the targets list of list\n",
    "    targ = targets[i]\n",
    "    print(f'No.{i+1}', end = '|')\n",
    "    for nucid in targ:\n",
    "        nucids.append(nucid)\n",
    "\n",
    "nucids = list(set(nucids)) # remove redundant if we have some\n",
    "print(f\"\\nTotal nucids {len(nucids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T08:04:30.463011Z",
     "start_time": "2017-09-03T08:04:30.452632Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadStep = 100 # this should not be changed during download\n",
    "start = 0 # should start from 0 if nothing have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T08:05:11.198493Z",
     "start_time": "2017-09-03T08:05:09.783690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last group No. 2327\n",
      "Now fetching groups from 2328 to 2327\n",
      "Group finished, already got 17839.13 MB data!\n",
      "Next download will start from group 2328 (between 0 - 2327).\n",
      "Before starting next query, please set how many groups you want to fetch based on your schedule.\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from time import strftime\n",
    "\n",
    "stepEnd = ceil(len(nucids)/downloadStep) # this is the last end number (ceil makes this number exclusive)\n",
    "print(f'Last group No. {str(stepEnd-1).zfill(4)}')\n",
    "\n",
    "numToFetch = 2328\n",
    "\n",
    "end = start + numToFetch # range(start, end) means not including end number!!\n",
    "\n",
    "if end > stepEnd:\n",
    "    end = stepEnd\n",
    "    \n",
    "print(f'Now fetching groups from {start} to {end-1}')\n",
    "\n",
    "gbfilePath = '/mnt/d/WORKs/temp/downloadingGenomes/'\n",
    "log_file = f'{gbfilePath}fetching.log'\n",
    "timestamp = strftime('%X %d/%m/%Y %Z')\n",
    "\n",
    "# write log file\n",
    "with open(log_file,'a') as log_handle:\n",
    "    log_handle.write(f'\\n{timestamp:*^50}\\nFrom {start*downloadStep+1} to {end*downloadStep+1-1} (inclusive)\\n{\"\":*^50}\\n')\n",
    "\n",
    "for i in range(start, stepEnd):\n",
    "    # Break the loop for shorter operation and debugging time\n",
    "    if i == end:\n",
    "        break\n",
    "    # Decide range of ids to fetch\n",
    "    id_start = i*downloadStep\n",
    "    id_end = (i+1)*downloadStep\n",
    "    file_Nu = str(i).zfill(4)\n",
    "    if id_end > len(nucids):\n",
    "        id_end = len(nucids)\n",
    "    ids = nucids[id_start:id_end]\n",
    "    \n",
    "    # Write note to screen and log file\n",
    "    logstr = f\"Fetching {file_Nu}: {ids[:4]}...({len(ids)})\"\n",
    "    print(logstr)\n",
    "    with open(log_file,'a') as log_handle:\n",
    "        log_handle.write(f'{logstr}\\n')\n",
    "    \n",
    "    output_file = f'{gbfilePath}stre_No_{file_Nu}.gb'\n",
    "    # Fetching...\n",
    "    with Entrez.efetch(db = 'nuccore',\n",
    "                       id = ids,\n",
    "                       rettype = 'gbwithparts',\n",
    "                       retmode = 'text'\n",
    "                      ) as handle:\n",
    "        with open(output_file, 'w') as out_handle:\n",
    "            out_handle.write(handle.read())\n",
    "    \n",
    "    # Write finishing note to screen and log file\n",
    "    logstr = f\"Finished {file_Nu}: {os.stat(output_file).st_size/1024/1024:.2f} MB {output_file} \\n\"\n",
    "    print(logstr)\n",
    "    with open(log_file,'a') as log_handle:\n",
    "        log_handle.write(f'{logstr}\\n')\n",
    "\n",
    "totalSize = 0 # calculate total amount data got from entrez\n",
    "for file in os.listdir(gbfilePath):\n",
    "    if file.endswith('.gb'):\n",
    "        totalSize += os.stat(os.path.join(gbfilePath,file)).st_size\n",
    "logstr = f'{totalSize/1024/1024:.2f} MB'\n",
    "logstr = f\"Group finished, already got {logstr} data!\"\n",
    "print(logstr)\n",
    "with open(log_file,'a') as log_handle:\n",
    "    log_handle.write(f'{logstr}\\n')\n",
    "\n",
    "start = end # ready for next round of fetching\n",
    "print(f'Next download will start from group {start} (between 0 - {stepEnd-1}).')\n",
    "print('Before starting next query, please set how many groups you want to fetch based on your schedule.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert genbank file to blast database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gb files needs to be converet to fasta file before making a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T08:24:24.944Z"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio import Seq\n",
    "from time import strftime\n",
    "timestamp = strftime('%X %d/%m/%Y %Z')\n",
    "\n",
    "print('Changing gb to fasta...')\n",
    "\n",
    "# Decide how many file to convert\n",
    "numFilesToConvert = 3000\n",
    "storage_folder_fa = '/mnt/d/WORKs/temp/downloadingGenomes/AllstrepFasta/'\n",
    "storage_folder_gb = '/mnt/d/WORKs/temp/downloadingGenomes/'\n",
    "\n",
    "# Setup starting value\n",
    "startNumFile = os.path.join(storage_folder_fa, 'temp')\n",
    "if os.path.isfile(startNumFile):\n",
    "    with open(startNumFile, 'rb') as handle:\n",
    "        start = pickle.load(handle)\n",
    "else:\n",
    "    start = 0\n",
    "    with open(startNumFile, 'wb') as handle:\n",
    "        pickle.dump(start, handle)\n",
    "end = start + numFilesToConvert\n",
    "totalGbs = sum(file.endswith('gb') for file in os.listdir(storage_folder_gb))\n",
    "if end > totalGbs:\n",
    "    end = totalGbs\n",
    "\n",
    "# write log file\n",
    "log_file = os.path.join(storage_folder_fa,'converting.log')\n",
    "with open(log_file,'a') as log_handle:\n",
    "    log_handle.write(f'\\n{timestamp:*^50}\\nFrom {start} to {end}\\n{\"\":*^50}\\n')\n",
    "\n",
    "for i in range(start,end):\n",
    "    num = str(i).zfill(4)\n",
    "    \n",
    "    input_file = os.path.join(storage_folder_gb, f'stre_No_{num}.gb') # This way I can check if there is a gap in fetched file number\n",
    "    output_file = os.path.join(storage_folder_fa, f'stre_No_{num}.fa')\n",
    "\n",
    "   \n",
    "    # Write note to screen and log file\n",
    "    logstr = f\"Converting stre_No_{num}.gb...\"\n",
    "    print(logstr)\n",
    "    with open(log_file,'a') as log_handle:\n",
    "        log_handle.write(f'{logstr}\\n')\n",
    " \n",
    "    records = SeqIO.parse(input_file, 'genbank')\n",
    "\n",
    "    with open(output_file, 'w') as fasta_out_handle:\n",
    "        num_empty = 0\n",
    "        recordWithSeq = []\n",
    "        for record in records:\n",
    "    #         print(type(record.seq))\n",
    "            if type(record.seq)==Seq.UnknownSeq: # Empty records will load as UnknownSeq\n",
    "                num_empty += 1\n",
    "                pass\n",
    "            else:\n",
    "                recordWithSeq.append(record)\n",
    "        if len(recordWithSeq) == 0: # if all records are empty, there is no point of writing it to fasta\n",
    "            logstr = f'There is no sequence in stre_No_{num}.gb, proceed to next file...'\n",
    "        else:\n",
    "            SeqIO.write(recordWithSeq, fasta_out_handle, 'fasta')\n",
    "            logstr = f\"Finished converting {num}, ignored {num_empty} empty records.\"\n",
    "        \n",
    "        print(logstr)\n",
    "        with open(log_file,'a') as log_handle:\n",
    "            log_handle.write(f'{logstr}\\n')\n",
    "\n",
    "# prepare for next round, set start number and dump to temp file        \n",
    "start = end\n",
    "pickle.dump(start, startNumFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make blast database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T14:12:12.598014Z",
     "start_time": "2017-09-03T14:12:12.589503Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T14:38:54.827732Z",
     "start_time": "2017-09-03T14:26:14.210771Z"
    }
   },
   "outputs": [],
   "source": [
    "sourceDir = '/mnt/d/WORKs/temp/downloadingGenomes/AllstrepFasta/'\n",
    "logFile = '/mnt/d/WORKs/temp/downloadingGenomes/blastdb/makeblastdb.log'\n",
    "outputDir = '/mnt/d/WORKs/temp/downloadingGenomes/blastdb/'\n",
    "\n",
    "totalNum = sum(file.endswith('fa') for file in os.listdir(sourceDir))\n",
    "print('Total number of files to be converted {totalNum}.\\nConverted:', end='')\n",
    "\n",
    "converted = 0\n",
    "for file in os.listdir(sourceDir):\n",
    "    if not file.endswith('fa'):\n",
    "        continue\n",
    "    gbFile = os.path.join(sourceDir, file)\n",
    "    args = ['makeblastdb',\n",
    "            '-in', gbFile,\n",
    "            '-input_type', 'fasta',\n",
    "            '-dbtype', 'nucl',\n",
    "            '-title', f'{file[:-3]}',\n",
    "            '-out', os.path.join(outputDir, f'{file[:-3]}'),\n",
    "            '-logfile', logFile,\n",
    "            '-taxid', '85011'\n",
    "           ]\n",
    "    run = subprocess.run(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    if run.returncode == 0:\n",
    "        converted += 1\n",
    "        print(f'{converted}|', end = '')\n",
    "        pass\n",
    "    else:\n",
    "        with open(logFile, 'r') as log:\n",
    "            for line in log.readlines():\n",
    "                print(line)\n",
    "        break\n",
    "print(f'\\n\\nFinished, databases made: {converted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge blast database into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T14:52:44.603809Z",
     "start_time": "2017-09-03T14:52:14.350395Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "databaseDir = '/mnt/d/WORKs/temp/downloadingGenomes/blastdb/'\n",
    "listFile = '/mnt/d/WORKs/temp/downloadingGenomes/blastdb/listofdbs'\n",
    "databaseList = []\n",
    "for file in os.listdir(databaseDir):\n",
    "    if not file.endswith('nsq'):\n",
    "        continue\n",
    "    dbName = file.split('.')[0]\n",
    "    databaseList.append(os.path.join(databaseDir, dbName))\n",
    "with open(listFile, 'w') as handle:\n",
    "    handle.write('\\n'.join(databaseList))\n",
    "    \n",
    "args = ['blastdb_aliastool',\n",
    "        '-dblist_file', listFile,\n",
    "        '-dbtype', 'nucl',\n",
    "        '-out', os.path.join(databaseDir, 'allStrepNucl20170902'),\n",
    "        '-title', 'allStrepNucl20170902',\n",
    "       ]\n",
    "run = subprocess.run(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "if run.returncode == 0:\n",
    "    pass\n",
    "else:\n",
    "    print(run.stdout.decode())\n",
    "    print(run.stderr.decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {
    "height": "62px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
